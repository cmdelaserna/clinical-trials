{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data into pySpark\n",
    "- https://github.com/titipata/pubmed_parser/wiki/Download-and-preprocess-MEDLINE-dataset\n",
    "\n",
    "- MEDLINE BULK DOWNLOAD -\n",
    "wget ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/*.gz\n",
    "\n",
    "- MEDLINE UPDATES -\n",
    "wget ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/*.gz\n",
    "- Change JAVA version: https://kodejava.org/how-do-i-set-the-default-java-jdk-version-on-mac-os-x/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import pubmed_parser as pp\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create spark session and variables\n",
    "sc = SparkContext(\"local\", \"medline db\")\n",
    "conf = SparkConf()\n",
    "spark = SparkSession.builder.\\\n",
    "    config(conf=conf)\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import medline files into a spark dataframe\n",
    "- https://github.com/titipata/pubmed_parser/wiki/Download-and-preprocess-MEDLINE-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medline_files_rdd = sc.parallelize(glob('../data/pubmed/*.gz'))\n",
    "\n",
    "medline_files_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_results_rdd = medline_files_rdd.\\\n",
    "    flatMap(lambda x: [Row(file_name=os.path.basename(x), **publication_dict) \n",
    "                       for publication_dict in pp.parse_medline_xml(x)])\n",
    "\n",
    "parse_results_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "medline_df = parse_results_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process upates and deletes\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rank, max, sum, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(['pmid']).orderBy(desc('file_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_df = medline_df.select(\n",
    "    max('delete').over(window).alias('is_deleted'),\n",
    "    rank().over(window).alias('pos'),\n",
    "    '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "medline_lastview = windowed_df.where('is_deleted = False and pos = 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(medline_lastview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore db\n",
    "- https://medium.com/@aieeshashafique/exploratory-data-analysis-using-pyspark-dataframe-in-python-bd55c02a2852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- is_deleted: boolean (nullable = true)\n",
      " |-- pos: integer (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- affiliations: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- chemical_list: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- delete: boolean (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- file_name: string (nullable = true)\n",
      " |-- issn_linking: string (nullable = true)\n",
      " |-- journal: string (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- medline_ta: string (nullable = true)\n",
      " |-- mesh_terms: string (nullable = true)\n",
      " |-- nlm_unique_id: string (nullable = true)\n",
      " |-- other_id: string (nullable = true)\n",
      " |-- pmc: string (nullable = true)\n",
      " |-- pmid: string (nullable = true)\n",
      " |-- pubdate: string (nullable = true)\n",
      " |-- publication_types: string (nullable = true)\n",
      " |-- references: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medline_lastview.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns\n",
    "columns = ['abstract', 'file_name', 'journal', 'keywords', 'mesh_terms', 'pubdate'] \n",
    "df = medline_lastview[columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- file_name: string (nullable = true)\n",
      " |-- journal: string (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      " |-- mesh_terms: string (nullable = true)\n",
      " |-- pubdate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting medline citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-7448bcdbfb02>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-7448bcdbfb02>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    wget \"$URL?db=PubMed&retmax=99&term=$QUERY\" -O - 2> /dev/null \\\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "wget \"$URL?db=PubMed&retmax=99&term=$QUERY\" -O - 2> /dev/null \\\n",
    "| grep \"^<Id>\" \\\n",
    "| sed -E 's|</?Id>||g' \\\n",
    "| cut -f3 \\\n",
    "> pmids.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate TF-IDF\n",
    "- https://github.com/titipata/pubmed_parser/wiki/Download-and-preprocess-MEDLINE-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify with Fastest\n",
    "- https://www.futurice.com/blog/classifying-text-with-fasttext-in-pyspark\n",
    "- https://fasttext.cc/docs/en/supervised-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "- https://nbviewer.jupyter.org/github/chambliss/Notebooks/blob/master/Word2Vec_News_Analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
